{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvectors and Eigenvalues\n",
    "\n",
    "Remember Matrices are linear transformations.\n",
    "\n",
    "To start, consider some linear transformation in two dimension. Moving the basis vectors $\\hat{i}$ to the coordinates of $[3,0]$ and $\\hat{j}$ to $[1,2]$. So its represented with a matrix such as:\n",
    "\n",
    "$$\\left[\\begin{array}{cc}3 & 1 \\\\ 0 & 2\\end{array}\\right]$$\n",
    "\n",
    "Focus in on what it does to one particular vector and think about the span of that vector, the line passing through its origin and its tip. Most vectors are going to get knocked off their span during the transformation, it would be somewhat coincidental if the place where the vector landed also happens to be somewhere on that line. But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it like a scalar. \n",
    "\n",
    "For this specific example, the basis vector $\\hat{i}$ is one such special vector, the span of $\\hat{i}$ is the x-axis and from the first column of the matrix, we can see that $\\hat{i}$ moves over to 3 times itself, still on the x-axis. \n",
    "\n",
    "Whats more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3 and hence remains on its own span. A slightly sneakier vector that remains on its own span during this transformation is $[-1,1]$. It ends up getting stretched by a factor of 2. And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. And for this transformation, those are all the vectors with this special property of staying on their span. Those on the x-axis getting stretched out by a factor of 3 and those on this diagonal line getting stretched by a factor of 2. Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. As you might of guessed by now, these special vectors are called the **eigenvectors** of the transformation, and each eigenvector has associated with it, whats called an **eigenvalue**, which is just the factor by which it stretched or squished during the transformation. Theres nothing special about stretching vs squishing or the fact that these eigenvalues happen to be positive. \n",
    "\n",
    "In another example, you could have an eigenvector with eigenvalue $[-\\frac{1}{2}]$, meaning that the vector gets flipped and squished by a factor of $\\frac{1}{2}$. But the important part here is that it stays on the line that it spans out without getting rotated off of it. \n",
    "\n",
    "For a glimpse of why this might be a useful thing to think about, consider some three dimensional rotation. If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the **axis of rotation**. Its much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which is rotating, rather than thinking about the full 3x3 matrix associated with that transformation. In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never squish anything so the length of the vector would remain the same. This pattern shows up a lot in linear algebra. \n",
    "\n",
    "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. But often a better way to get at the hear of what the linear transformation actually does less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. \n",
    "\n",
    "Symbolically, heres what the idea of an eigenvector looks like.\n",
    "\n",
    "$$A\\vec{v} = \\lambda\\vec{v}$$\n",
    "\n",
    "A is the matrix representing some transformation, with v as the eigenvector, and $\\lambda$ is a number, namely the corresponding eigenvalue. What this expression is saying is that the matrix-vector product $A$ times $\\vec{v}$ gives the same result as just scaling the eigenvector $\\vec{v}$ by some value $\\lambda$ (scalar multiplication). So finding the eigenvectors and their eigenvalues of a matrix A, comes down to finding the values of $\\vec{v}$ and $\\lambda$ that make this expression true. \n",
    "\n",
    "Its a little awkward because the left-hand side represents matrix-vecttor multiplication, but the right hand side is scalar-vector multiplication. So let's start by rewriting the right hand side as some kind of matrix-vector multiplication, using a matrix, which has the effect of scaling any vector by a factor of $\\lambda$. The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply times $\\lambda$. So this matrix will have the number $\\lambda$ down the diagonal with 0's everywhere else.\n",
    "\n",
    "$$\\left[\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda \\end{array}\\right]$$\n",
    "\n",
    "The common way to write this guy is to factor that $\\lambda$ out and write it as $\\lambda$ times $I$, where $I$ is the identity matrix with 1s down the diagonal.\n",
    "\n",
    "$$A\\vec{v} = \\left(\\lambda I\\right)\\vec{v}$$\n",
    "\n",
    "With both sides looking like matrix-vector multiplication, we can subtract off that right hand side and factor out the v. \n",
    "\n",
    "$$A\\vec{v} - \\left(\\lambda I\\right)\\vec{v} = 0$$\n",
    "$$\\left(A - \\lambda I\\right)\\vec{v} = 0$$\n",
    "\n",
    "So what we now have is a new matrix - A minus $\\lambda$ times the identity, and we're looking for a vector v, such that this new matrix times v gives the zero vector. Now this will always be true if v itself is the zero vector, what we want is a non-zero eigenvector. \n",
    "\n",
    "And we know that the only way its possible for tthe product of a mattrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.\n",
    "\n",
    "$$\\text{det}\\left(A - \\lambda I\\right) = 0$$\n",
    "\n",
    "And that squishificatiton corresponds to a zero detterminant for that matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenvalues = np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.41421356, 2.        , 0.58578644])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
