{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvectors and Eigenvalues\n",
    "\n",
    "Remember Matrices are linear transformations.\n",
    "\n",
    "To start, consider some linear transformation in two dimension. Moving the basis vectors $\\hat{i}$ to the coordinates of $[3,0]$ and $\\hat{j}$ to $[1,2]$. So its represented with a matrix such as:\n",
    "\n",
    "$$\\left[\\begin{array}{cc}3 & 1 \\\\ 0 & 2\\end{array}\\right]$$\n",
    "\n",
    "Focus in on what it does to one particular vector and think about the span of that vector, the line passing through its origin and its tip. Most vectors are going to get knocked off their span during the transformation, it would be somewhat coincidental if the place where the vector landed also happens to be somewhere on that line. But some special vectors do remain on their own span, meaning the effect that the matrix has on such a vector is just to stretch it or squish it like a scalar. \n",
    "\n",
    "For this specific example, the basis vector $\\hat{i}$ is one such special vector, the span of $\\hat{i}$ is the x-axis and from the first column of the matrix, we can see that $\\hat{i}$ moves over to 3 times itself, still on the x-axis. \n",
    "\n",
    "Whats more, because of the way linear transformations work, any other vector on the x-axis is also just stretched by a factor of 3 and hence remains on its own span. A slightly sneakier vector that remains on its own span during this transformation is $[-1,1]$. It ends up getting stretched by a factor of 2. And again, linearity is going to imply that any other vector on the diagonal line spanned by this guy is just going to get stretched out by a factor of 2. And for this transformation, those are all the vectors with this special property of staying on their span. Those on the x-axis getting stretched out by a factor of 3 and those on this diagonal line getting stretched by a factor of 2. Any other vector is going to get rotated somewhat during the transformation, knocked off the line that it spans. As you might of guessed by now, these special vectors are called the **eigenvectors** of the transformation, and each eigenvector has associated with it, whats called an **eigenvalue**, which is just the factor by which it stretched or squished during the transformation. Theres nothing special about stretching vs squishing or the fact that these eigenvalues happen to be positive. \n",
    "\n",
    "In another example, you could have an eigenvector with eigenvalue $[-\\frac{1}{2}]$, meaning that the vector gets flipped and squished by a factor of $\\frac{1}{2}$. But the important part here is that it stays on the line that it spans out without getting rotated off of it. \n",
    "\n",
    "For a glimpse of why this might be a useful thing to think about, consider some three dimensional rotation. If you can find an eigenvector for that rotation, a vector that remains on its own span, what you have found is the **axis of rotation**. Its much easier to think about a 3D rotation in terms of some axis of rotation and an angle by which is rotating, rather than thinking about the full 3x3 matrix associated with that transformation. In this case, by the way, the corresponding eigenvalue would have to be 1, since rotations never squish anything so the length of the vector would remain the same. This pattern shows up a lot in linear algebra. \n",
    "\n",
    "With any linear transformation described by a matrix, you could understand what it's doing by reading off the columns of this matrix as the landing spots for basis vectors. But often a better way to get at the hear of what the linear transformation actually does less dependent on your particular coordinate system, is to find the eigenvectors and eigenvalues. \n",
    "\n",
    "Symbolically, heres what the idea of an eigenvector looks like.\n",
    "\n",
    "$$A\\vec{v} = \\lambda\\vec{v}$$\n",
    "\n",
    "A is the matrix representing some transformation, with v as the eigenvector, and $\\lambda$ is a number, namely the corresponding eigenvalue. What this expression is saying is that the matrix-vector product $A$ times $\\vec{v}$ gives the same result as just scaling the eigenvector $\\vec{v}$ by some value $\\lambda$ (scalar multiplication). So finding the eigenvectors and their eigenvalues of a matrix A, comes down to finding the values of $\\vec{v}$ and $\\lambda$ that make this expression true. \n",
    "\n",
    "Its a little awkward because the left-hand side represents matrix-vecttor multiplication, but the right hand side is scalar-vector multiplication. So let's start by rewriting the right hand side as some kind of matrix-vector multiplication, using a matrix, which has the effect of scaling any vector by a factor of $\\lambda$. The columns of such a matrix will represent what happens to each basis vector, and each basis vector is simply times $\\lambda$. So this matrix will have the number $\\lambda$ down the diagonal with 0's everywhere else.\n",
    "\n",
    "$$\\left[\\begin{array}{ccc}\\lambda & 0 & 0 \\\\ 0 & \\lambda & 0 \\\\ 0 & 0 & \\lambda \\end{array}\\right]$$\n",
    "\n",
    "The common way to write this guy is to factor that $\\lambda$ out and write it as $\\lambda$ times $I$, where $I$ is the identity matrix with 1s down the diagonal.\n",
    "\n",
    "$$A\\vec{v} = \\left(\\lambda I\\right)\\vec{v}$$\n",
    "\n",
    "With both sides looking like matrix-vector multiplication, we can subtract off that right hand side and factor out the v. \n",
    "\n",
    "$$A\\vec{v} - \\left(\\lambda I\\right)\\vec{v} = 0$$\n",
    "$$\\left(A - \\lambda I\\right)\\vec{v} = 0$$\n",
    "\n",
    "So what we now have is a new matrix - A minus $\\lambda$ times the identity, and we're looking for a vector v, such that this new matrix times v gives the zero vector. Now this will always be true if v itself is the zero vector, what we want is a non-zero eigenvector. \n",
    "\n",
    "And we know that the only way its possible for tthe product of a mattrix with a non-zero vector to become zero is if the transformation associated with that matrix squishes space into a lower dimension.\n",
    "\n",
    "$$\\text{det}\\left(A - \\lambda I\\right) = 0$$\n",
    "\n",
    "And that squishificatiton corresponds to a zero detterminant for that matrix. To be concrete, let's say your matrix $A$ has columns $[2,1]$ and $[2,3]$. \n",
    "\n",
    "$$\\left[\\begin{array}{cc}2 - 0.00 & 2 \\\\ 1 & 3 - 0.00 \\end{array}\\right] --- (A - \\lambda I) $$\n",
    "\n",
    "Think about subtracting off a variable amount $\\lambda$ from each diagonal entry. Now imagine tweaking $\\lambda$, turning a knob to change its value. As that value of $\\lambda$ changes, the matrix itself changes, and so the determinant of the matrix changes. The goal here is to find a value of $\\lambda$ that will make this determinant zero, meaning the tweaked transformation squishes space into a lower dimension. In this case, the sweet spot comes when $\\lambda$ equals 1. Of course, if we choose some other matrix, the eigenvalue might not necessarily be 1, the sweet spot might be hit some other value of $\\lambda$. Let's unravel what this is saying. \n",
    "\n",
    "$$\\text{det}\\left(\\left[\\begin{array}{cc}2 - 1.00 & 2 \\\\ 1 & 3 - 1.00 \\end{array}\\right]\\right) = 0.00 $$\n",
    "\n",
    "When $\\lambda$ equals 1, the matrix $\\mathbf{A}$ minus $\\lambda$ times the identity squishes space onto a line. That means theres a non-zero vector $\\vec{v}$, such that $\\mathbf{A}$ minus $\\lambda$ times the identity times $\\vec{v}$ equals the zero vector. And remember, the reason we care about that is because it meas $A\\vec{v} = (\\lambda I)\\vec{v}$, which we can read off as saying that the vector $\\vec{v}$ is an eigenvector of $\\mathbf{A}$, staying on its own span during the transformation $A$. In this example, the cooresponding eigenvalue is 1, so $\\vec{v}$ would actually just stay fixed in place. \n",
    "\n",
    "$$\\begin{align}\n",
    "    A\\vec{v} & = \\lambda\\vec{v} \\\\\n",
    "    A\\vec{v} - \\lambda I \\vec{v} & = 0 \\\\\n",
    "    \\left( A - \\lambda I \\right)\\vec{v} &= 0 \\\\\n",
    "    \\text{det}\\left( A - \\lambda I \\right) & = 0\n",
    "  \\end{align}$$\n",
    "\n",
    "We need a solid grasp of determinants and why they relate to linear systems of equations having non-zero solutions. An expression like this would feel completely out of the blue. To see the bottom equation in action, lets revisit the example from the start with a matrix whose columns are $[3,0]$ and $[1,2]$. To find if a value $\\lambda$ is an eigenvalue, subtract it from the diagonals of this matrix and compute the determinant. \n",
    "\n",
    "$$\\left[\\begin{array}{cc}3 & 1 \\\\ 0 & 2 \\end{array}\\right]$$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}3 - \\lambda & 1 \\\\ 0 & 2 - \\lambda \\end{array}\\right]$$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}3 - \\lambda & 1 \\\\ 0 & 2 - \\lambda \\end{array}\\right]$$\n",
    "\n",
    "$$\\text{det}\\left(\\left[\\begin{array}{cc}3 - \\lambda & 1 \\\\ 0 & 2 - \\lambda \\end{array}\\right]\\right)$$\n",
    "\n",
    "\n",
    "Doing this, we get a certain quadratic polynomial in $\\lambda$\n",
    "\n",
    "$$\\text{det}\\left(\\left[\\begin{array}{cc}3 - \\lambda & 1 \\\\ 0 & 2 - \\lambda \\end{array}\\right]\\right) = \\left(3-\\lambda\\right)\\left(2-\\lambda\\right)$$.\n",
    "\n",
    "Recall to compute a determinant, we need a square matrix, and the \n",
    "\n",
    "Since $\\lambda$ can only be an eigenvalue if this determinant happens to be zero, you can conclude that the only possible eigenvalues are $\\lambda = 2$ and $\\lambda = 3$. To figure out what the eigenvectors are that actually have one of these eigenvalues say $\\lambda = 2$, plug in that value of $\\lambda$ to the matrix and then solve for which vectors this diagonally altered matrix sends to 0. \n",
    "\n",
    "$$\\left[\\begin{array}{cc}3 - 2 & 1 \\\\ 0 & 2 - 2 \\end{array}\\right] \\left[\\begin{array}{c} x \\\\ y \\end{array}\\right] = \\left[\\begin{array}{c} 0 \\\\ 0 \\end{array}\\right]$$\n",
    "\n",
    "If you computed this the way you would any other linear system, you'd see that the solutions are all the vectors on the diagonal line spanned by $(-1, 1)$. This corresponds to the fact that the unaltered matrix $[(3,0),(1,2)]$. Has the effect of stretching all those vectors by a factor of 2. \n",
    "\n",
    "Now, a 2D transformation doesn't have to have eigenvectors. **There could be no eigenvectors**. For example, consider a rotation by 90 degrees, this doesn't have any eigenvectors, since it rotates every vector off of its own span. If you actually try computing the eigenvalues of a rotation like this, notice what happens. Its matrix has columns $(0,1)$, and $(-1,0)$. Subtrack off $\\lambda$ from the diagonal elements and look for when the determinant is 0. \n",
    "\n",
    "$$\\text{det}\\left(\\left[\\begin{array}{cc}- \\lambda & -1 \\\\ 1 & - \\lambda \\end{array}\\right]\\right) = (-\\lambda)(-\\lambda) - (-1)(1) $$\n",
    "\n",
    "In this case you get the polynomial $\\lambda^2 + 1$, the only roots of that polynomial are the imaginary numbers $i$, and $-i$. The fact that thre are no real number solutions indicates that there are no eigenvectors. \n",
    "\n",
    "### Shear Example\n",
    "Another pretty interesting example worth holding in the back of your mind is a shear. This fixes $\\hat{i}$ in place and moves $\\hat{j}$ one over, so its matrix has columns:\n",
    "\n",
    "$$\\left[\\begin{array}{cc}1 & 1 \\\\ 0 & 1 \\end{array}\\right]$$\n",
    "\n",
    "All of the vectors on the x-axis are eigenvectors with eigenvalue 1, since they remain fixed in place. In fact, these are the only eigenvectors, when you subtract off $\\lambda$ from the diagonals and compute the determinant, \n",
    "\n",
    "$$\\text{det}\\left(\\left[\\begin{array}{cc} 1- \\lambda & 1 \\\\ 0 & 1- \\lambda \\end{array}\\right]\\right) = (1-\\lambda)(1-\\lambda) - (0)(1)$$\n",
    "\n",
    "The only root of this expression is $\\lambda = 1$, this lines up with what we see geometrically that all of the eigenvectors have eigenvalue 1. Keep in mind though, its also possible to have just one eigenvalue, but with more than just a line full of eigenvectors. A simple example is a matrix that scales everything by 2, the only eigenvalue is 2, but every other vector in the plane gets to be an eigenvector with that eigenvalue. \n",
    "\n",
    "$$\\left[\\begin{array}{cc} 2 & 0 \\\\ 0 & 2 \\end{array}\\right]$$ \n",
    "\n",
    "### Eigen Basis\n",
    "\n",
    "**What if both basis vectors are eigenvectors?**\n",
    "\n",
    "Take a look at what happens if our basis vectors just so happened to be eigenvectors, for example, maybe $\\hat{i}$ is scaled by -1 and $\\hat{j}$ is scaled by 2. Notice that those scalar multiplies $-1,2$ which are the eigenvalues of $\\hat{i}$ and $\\hat{j}$, sit on the diagonal of our matrix and every other entry is a 0. \n",
    "\n",
    "$$\\left[\\begin{array}{cc} -1 & 0 \\\\ 0 & 2 \\end{array}\\right]$$ \n",
    "\n",
    "Anytime a matrix has 0's everywhere other than the diagonal, its called reasonably enough, a **diagonal matrix**. And a way to interpret this is that all the basis vectors are eigenvectors, with the diagonal entries of this matrix being their eigenvalues. \n",
    "\n",
    "$$\\left[\\begin{array}{cccc} -5 & 0 & 0 & 0 \\\\ 0 & -2 & 0 & 0 \\\\ 0 & 0 & -4 & 0 \\\\ 0 & 0 & 0 & 4 \\end{array}\\right]$$ \n",
    "\n",
    "There are a lot of things that make diagonal matrices much nicer to work with. One big one is that its easier to compute what will happen if you multiply the below matrix by itself a whole bunch of times.\n",
    "\n",
    "$$\\left[\\begin{array}{cc} 3 & 0 \\\\ 0 & 2 \\end{array}\\right]\\left[\\begin{array}{cc} x \\\\ y \\end{array}\\right]$$ \n",
    "\n",
    "Becomes,where $n$ is the number you times are multiplying the above matrix by itself.\n",
    "\n",
    "$$\\left[\\begin{array}{cc} 3x^n \\\\ 2y^n \\end{array}\\right]$$\n",
    "\n",
    "Since all one of these matrices does is scale each basis vector by some eigenvalue, applying that matrix many times, say 100 times, is just going to coorespond to scaling each basis vector by the 100th power of the cooresponding eigenvalue. \n",
    "\n",
    "$$\\left[\\begin{array}{cc} 3^{100} & 0 \\\\ 0 & 2^{100} \\end{array}\\right]\\left[\\begin{array}{cc} x \\\\ y \\end{array}\\right]$$\n",
    "\n",
    "In contrast, try computing the 100th power of a non-diagonal matrix. It is a nightmare. But, of course, you will rarely be so lucky as to have your basis vectors also be eigenvectors, but if your transformation has a lot of eigenvectors, like the one from the start of this lesson, enough so that you can choose a set that spans the full space, then you could change your coordinate system to that these eigenvectors are you basis vectors. \n",
    "\n",
    "Recall how to express a transformation currently written in our coordinate system into a different system. Take the coordinates of the vectors that you want to use as a new basis.\n",
    "\n",
    "$$\\left[\\begin{array}{cc} 3 & 1 \\\\ 0 & 2 \\end{array}\\right]$$\n",
    "\n",
    "which, in this case, means our two eigenvectors, that make those coordinates the columns of a matrix, known as the change of basis matrix.\n",
    "\n",
    "$$\\left[\\begin{array}{cc} 3 & 1 \\\\ 0 & 2 \\end{array}\\right]\\left[\\begin{array}{cc} 1 & -1 \\\\ 0 & 1 \\end{array}\\right]$$\n",
    "\n",
    "When you sandwich the original transformation matrix putting the change of basis matrix on its right, and the inverse of the change of basis matrix on its left, the result will be a matrix representing that same transformation, but from the perspective of the new basis vectors coordinate system. \n",
    "\n",
    "$$\\left[\\begin{array}{cc} 1 & -1 \\\\ 0 & 1 \\end{array}\\right]^{-1}\\left[\\begin{array}{cc} 3 & 1 \\\\ 0 & 2 \\end{array}\\right]\\left[\\begin{array}{cc} 1 & -1 \\\\ 0 & 1 \\end{array}\\right]$$\n",
    "\n",
    "The whole point of doing this with eigenvectors is that this new matrix is guaranteed to be diagonal with its cooresponding eigenvalues down that diagonal. This is because it represents working in a coordinate system where what happens to the basis vectors is that they get scaled during the transformation. A set of basis vectors, which are also eigenvectors, is called an **eigenbasis**. \n",
    "\n",
    "$$\\left[\\begin{array}{cc} 1 & -1 \\\\ 0 & 1 \\end{array}\\right]^{-1}\\left[\\begin{array}{cc} 3 & 1 \\\\ 0 & 2 \\end{array}\\right]\\left[\\begin{array}{cc} 1 & -1 \\\\ 0 & 1 \\end{array}\\right] = \\left[\\begin{array}{cc} 3 & 0 \\\\ 0 & 2 \\end{array}\\right]$$\n",
    "\n",
    "So if for example, you needed to compute the 100th power of this matrix, it would be much easier to change to an eigenbasis, compute the 100th power in that system, then convert back to our standard system. \n",
    "\n",
    "Not all matrices can become diagonal, a shear, for example, doesn't have enough eigenvectors to span the full space. But if you can find an eigenbasis, it makes matrix operations really lovely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[0,1],[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenvalues = np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61803399,  1.61803399])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rotation_matrix = np.matrix([[3, 1], [-1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.61803399,  1.61803399]), matrix([[-0.85065081, -0.52573111],\n",
       "         [ 0.52573111, -0.85065081]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
