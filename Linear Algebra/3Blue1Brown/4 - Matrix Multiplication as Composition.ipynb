{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication as Composition\n",
    "\n",
    "\n",
    "### Linear Transformation Recap\n",
    "Linear Transformations are functions, with vectors as inputs, and vectors as outputs. We can think about them visually as smooshing around space in such a way the gridlines stay parallel and evenly spaced, and so that the origin remains fixed. The key take-away was that a linear transformation is completely determined by where it takes the basis vectors of the space, which for two dimensions means $\\hat{i}$ and $\\hat{j}$. This is because any other vector can be described as a linear combination of those basis vectors. A vector with coordinates $(x, y)$ is x times $\\hat{i}$ plus y times $\\hat{j}$. \n",
    "\n",
    "$$ L(\\vec{v})$$\n",
    "\n",
    "The place where your vector lands will be x times the transformed version of $\\hat{i}$ + y times the transformed version of $\\hat{j}$. This means if you keep a record of the coordinates where $\\hat{i}$ lands and the coordinates where $\\hat{j}$ lands  you can compute that a vector which starts at $(x, y)$ must land on x times the new coordinates of $\\hat{i}$ + y times the new coordinates of $\\hat{j}$.\n",
    "\n",
    "The convention is to record the coordinates of where $\\hat{i}$ and $\\hat{j}$ land as the columns of a matrix and to define this sum of the scaled versions of thos columns by x and y to be matrix-vector multiplication.  \n",
    "\n",
    "$$\\left[ \\begin{array}{cc} a & b\\\\ c & d \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} x\\\\ y \\\\ \\end{array} \\right] = x \\left[ \\begin{array}{c} a\\\\ c \\\\ \\end{array} \\right] + y \\left[ \\begin{array}{c} b\\\\ d \\\\ \\end{array} \\right] $$\n",
    "\n",
    "A matrix represents a specific linear transformation and multiplying a matrix by a vector is, what it means computationally, to apply that transformation to that vector. \n",
    "\n",
    "### Matrix Multiplication Composition\n",
    "\n",
    "Often times you find youself wanting to describe the effect of applying one transformation and then another. For example, maybe we want to describe what happens when you first rotate the plane $90^o$ counterclockwise, then apply a shear. The overall effect here, from start to finish, is another linear transformation distinct from the rotation and the shear. This new linear transformation is commonly called the \"composititon\" of the two separate transformations we applied. And like any linear transformation it can be described with a matrix all of its own, by following $\\hat{i}$ and $\\hat{j}$. \n",
    "\n",
    "By looking at the final locations of where $\\hat{i}$ and $\\hat{j}$ land we can discover the matrix of the composition of both linear transformations. This matrix when applied to the initial vector will provide an output of the final effects of both linear transformations. The long way to compute this effect is to first multiply it on the left by the rotation matrix:\n",
    "\n",
    "$$\\left[ \\begin{array}{cc} 0 & -1\\\\ 1 & 0 \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} x\\\\ y \\\\ \\end{array} \\right]$$\n",
    "\n",
    "Then to take whatever you get and multiply that on the left by the sheer matrix. \n",
    "\n",
    "**Fig 2:**\n",
    "$$\\left[ \\begin{array}{cc} 1 & 1\\\\ 0 & 1 \\\\ \\end{array} \\right] \\left(\\left[ \\begin{array}{cc} 0 & -1\\\\ 1 & 0 \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} x\\\\ y \\\\ \\end{array} \\right]\\right)$$\n",
    "\n",
    "But, whatever you get should be the same as just applying this new composition matrix that we just found, by that same vector, no matter what vector you choose, since this new matrix is supposed to capture the same overall effect as the rotation-then-sheer action. Thus, we can consider this composition to be the **product** of both matrixes. Always remember, that multiplying two matrices like this has the geometric meaning of applying one transformation then another. \n",
    "\n",
    "One thing that we can see from **fig 2** is that it has us reading from right to left, first applying the transformation represented by the matrix on the right. Then you apply the transformation represented by the matrix on the left. This stems from function notation, since we write functions on the left of variables $f(g(x))$, so every time you compose two functions, you always have to read it right to left. \n",
    "\n",
    "**Example**\n",
    "\n",
    "Take the Matrix\n",
    "$M_1 = \\left[ \\begin{array}{cc} 1 & -2\\\\ 1 & 0 \\\\ \\end{array} \\right], M_2 = \\left[ \\begin{array}{cc} 0 & 2\\\\ 1 & 0 \\\\ \\end{array} \\right]$, the total effect of applying $M_1$ then $M_2$ is called the composition matrix of these linear transformations. \n",
    "\n",
    "Or Mathematically, $\\left[ \\begin{array}{cc} 0 & 2\\\\ 1 & 0 \\\\ \\end{array} \\right]\\left[ \\begin{array}{cc} 1 & -2\\\\ 1 & 0 \\\\ \\end{array} \\right]$ gives us a new transformation, so let's find its matrix. \n",
    "\n",
    "First, we need to figure out where $\\hat{i}$ goes? After applying $M_1$ the new coordinates of $\\hat{i}$ by definition, are given by the first column of $M_1$, namely $(1, 1)$. To see what happens after applying $M_2$ we multiply the matrix for $M_2$ by that vector $(1,1)$ Thus, mathematically, $\\left[ \\begin{array}{cc} 0 & 2\\\\ 1 & 0 \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} 1\\\\ 1 \\\\ \\end{array} \\right]$, this equates to $ = 1 \\left[ \\begin{array}{c} 0\\\\ 1 \\\\ \\end{array} \\right] + 1 \\left[ \\begin{array}{c} 2\\\\ 0 \\\\ \\end{array} \\right]$, we will get the vector $ \\left[ \\begin{array}{c} 2\\\\ 1  \\\\ \\end{array} \\right]$. This will be the first column of the composition matrix. \n",
    "\n",
    "Likewise, to follow $\\hat{j}$, the second column of $M_1$ tells us that it first lands on $(-2, 0)$, then when we apply $M_2$ to that vector $\\left[ \\begin{array}{cc} 0 & 2\\\\ 1 & 0 \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} -2\\\\ 0 \\\\ \\end{array} \\right]$, you can work out the matrix-vector product $-2 \\left[ \\begin{array}{c} 0\\\\ 1 \\\\ \\end{array} \\right] + 0 \\left[ \\begin{array}{c} 2\\\\ 0 \\\\ \\end{array} \\right]$to get $(0, -2)$. This is the second column of our composition matrix.\n",
    "\n",
    "Thus the final composition matrix after both applying $M_1$ then $M_2$ is, $\\left[ \\begin{array}{cc} 2 & 0 \\\\ 1 & 0 \\\\ \\end{array} \\right]$\n",
    "\n",
    "This line of reasoning works for any matrices. \n",
    "\n",
    "**General Purpose**\n",
    "\n",
    "$$\\left[ \\begin{array}{cc} a & b\\\\ c & d \\\\ \\end{array} \\right] \\left[ \\begin{array}{cc} e & f\\\\ g & h \\\\ \\end{array} \\right]$$\n",
    "\n",
    "To follow where $\\hat{i}$ goes, we start by looking at the first column of the matrix on the right, since this is where $\\hat{i}$ initially lands. Multiplying that column by the matrix on the left, is how you can tell where the immediate version of $\\hat{i}$ ends up after applying the second transformation. The first column of the composition matrix will always equal the left matrix times the first column of the right matrix, as shown below in **fig 1**. Likewise, $\\hat{j}$ will always initially land on the second column of the right matrix. So multiplying the left matrix by this second column will give its final location and hence, that is the second column of the composition matrix. This can be seen in **Fig 2**.\n",
    "\n",
    "**Fig 1: Finding $\\hat{i}$ Location**\n",
    "$$\\left[ \\begin{array}{cc} a & b\\\\ c & d \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} e\\\\ g \\\\ \\end{array} \\right] = e\\left[ \\begin{array}{c} a\\\\ c \\\\ \\end{array} \\right] + g\\left[ \\begin{array}{c} b\\\\ d \\\\ \\end{array} \\right] = \\left[ \\begin{array}{cc} ae + bg\\\\ ce + dg \\\\ \\end{array} \\right]$$\n",
    "\n",
    "**Fig 2: Finding $\\hat{j}$ Location**\n",
    "$$\\left[ \\begin{array}{cc} a & b\\\\ c & d \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} f\\\\ h \\\\ \\end{array} \\right] = f \\left[ \\begin{array}{c} a\\\\ c \\end{array} \\right] + h\\left[ \\begin{array}{c} b\\\\ d \\end{array} \\right] = \\left[ \\begin{array}{cc} af + bh\\\\ cf + dh \\\\ \\end{array} \\right]$$\n",
    "\n",
    "Yes, there's a lot of symbols and may be confusing but to really understand what is going on here, its best to understand what is matrix multiplication, which is applying one transformation after another. \n",
    "\n",
    "\n",
    "### **Questions**\n",
    "\n",
    "##### Does it matter what order we put the two matrices in when we multiply them? \n",
    "\n",
    "**Yes, order totally does matter**\n",
    "\n",
    "$$ \\mathbf{M}_1 \\mathbf{M}_2 \\ne \\mathbf{M}_2 \\mathbf{M}_1 $$\n",
    "\n",
    "\n",
    "##### Is matrix multiplication associative? \n",
    "\n",
    "**Yes, Matrix Multiplication is Associative.**\n",
    "\n",
    "$$ \\left(AB\\right)C = A\\left(BC\\right)$$\n",
    "\n",
    "This means that if you have three matrices and you multiply them together it shouldn't matter if you first compute C times B times A. But if you recall matrix multiplication is just applying one transformation after another. What it is saying is that if you first apply C then B, then A, its the same as applying C, then B, then A. There is nothing to prove as you're just applying the same three things one after the other all in the same order. This is a proof that matrix multiplication is associative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m1 = np.column_stack([[1, -2], [1, 0]])\n",
    "m2 = np.column_stack([[0, 2], [1, 0]])\n",
    "\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 0, -2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 @ m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1[1][0] * m2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2,  0],\n",
       "       [ 2,  2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 @ m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
