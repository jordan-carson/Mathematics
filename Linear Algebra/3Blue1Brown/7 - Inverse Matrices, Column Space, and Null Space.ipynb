{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Matrices, Column Space, Rank and Null Space\n",
    "\n",
    "The bulk of this series is on understanding matrix matrix and vector operations through the more visual lens of linear transformations. \n",
    "\n",
    "We will be describing the intiution behind the following:\n",
    "\n",
    "- Inverse Matrices\n",
    "- Column Space\n",
    "- Rank\n",
    "- Null Space\n",
    "\n",
    "\"Guassian Elimination\", and \"Row Echelon Form\"\n",
    "\n",
    "One of the main reasons Linear Algebra is more broadly applicable is it allows us to solve certain systems of equations. You have a list of variables, things you don't know, and a list of equations solving them. \n",
    "\n",
    "$$\\begin{align}\n",
    " 2x + 5y + 3z & = -3 \\\\\n",
    " 4x + 0y + 8z &= 0 \\\\\n",
    " 1x + 3y + 0z & = 2 \\\\\n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "Within each equation, the only thing happening to each variable is that it's scaled by some constant, and the only thing happening to each of those scaled variables is that they're added to each other. Thus, no exponents ($x^2$) or fancy functions like ($\\sin{x}$), or multiplication of two variables together. The way to organize this special system is to throw all the variables on the left, and put any lingering constants on the right. We also vertically line up the common variables, such as x over x, and to do this we may need to throw in some zero coefficients. This may look a lot like matrix-vector multiplication in fact, we can package all of the equations together into a single vector equation. \n",
    "\n",
    "$$\\left[\\begin{array}{ccc}\n",
    "    2 & 5 & 3 \\\\\n",
    "    4 & 0 & 8 \\\\\n",
    "    1 & 3 & 0 \\\\\n",
    "    \\end{array}\\right]\n",
    "    \\left[\\begin{array}{c}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\\\\\n",
    "    \\end{array}\\right]\n",
    "   =\n",
    "    \\left[\\begin{array}{c}\n",
    "    -3 \\\\\n",
    "    0 \\\\\n",
    "    2 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "    \n",
    "Where you have the matrix containing all of the constant coefficients, and a vector containing all of the variables, and their matrix vector product equals some different constant vector. Lets name the constant matrix $A$, the vector holding the variables with a $\\mathbf{\\vec{x}}$, and the constant vector on the right-hand side $\\vec{v}$.\n",
    "\n",
    "$$A \\vec{x} = \\vec{v}$$\n",
    "\n",
    "The matrix A corresponds with some linear transformation, so solving $A \\vec{x} = \\vec{v}$, means we're looking for a vector $\\vec{x}$ which, after applying the transformation, lands on $\\vec{v}$. You can hold in your head this really complicated idea of multiple variables all intermingling with each other just by thinking about squishing and morphing space and trying to figure out which vector lands on another. \n",
    "\n",
    "To start simple, let's say you have a system with two equations and two unknowns. \n",
    "\n",
    "**Fig 1: Two Unknown - System of Equations**\n",
    "$$\\begin{align}\n",
    " 2x + 2y & = -4 \\\\\n",
    " 1x + 3y &= -1 \\\\\n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "This means that the matrix $A$ is a 2x2 matrix, and $\\vec{v}$ and $\\vec{x}$ are each two dimensional vectors. \n",
    "\n",
    "$$\\left[\\begin{array}{cc}\n",
    "    2 & 2 \\\\\n",
    "    1 & 3 \\\\\n",
    "    \\end{array}\\right]\n",
    "    \\left[\\begin{array}{c}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    \\end{array}\\right]\n",
    "   =\n",
    "    \\left[\\begin{array}{c}\n",
    "    -4 \\\\\n",
    "    -1 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "\n",
    "\n",
    "Now, how we think about the solutions to this equations depends on whether the transformation associated with $A$ squishes all of space into a lower dimension, like a line or a point, or if it leaves everything spanning the full two dimensions where it started. In the language of the previous video, we subdivide into the case where $A$ has zero determinant, and the case where A has nonzero determinant. \n",
    "\n",
    "**If the determinant is nonzero**\n",
    "If the determinant is nonzero meaning space does not get squished into a zero area region. In this case, there will always be one and only one vector that lands on $\\vec{v}$, and you can find it by playing the transformation in reverse. Following where $\\vec{v}$ goes you'll find the vector $\\vec{x}$ such that $A$ times $\\vec{x}$ equals $\\vec{v}$. $A \\vec{x} = \\vec{v}$\n",
    "When you play the transformation in reverse it actually corresponds to a separate linear transformation, commonly called the **inverse of A**. Denoted $\\mathbf{A}^{-1}$. \n",
    "\n",
    "**For examples **\n",
    "\n",
    "If $A$ was a counterclockwise rotation by 90 degrees, then the inverse of A would be clockwise rotation of 90 degrees\n",
    "$$\\left[\\begin{array}{cc}\n",
    "    0 & -1 \\\\\n",
    "    1 & 0 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "    \n",
    "Then the inverse of A would be a clockwise rotation by 90 degrees. \n",
    "$$\\left[\\begin{array}{cc}\n",
    "    0 & 1 \\\\\n",
    "    -1 & 0 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "\n",
    "If $A$ was a rightward shear that pushes $\\hat{j}$ one unit to the right,\n",
    "$$\\left[\\begin{array}{cc}\n",
    "    1 & 1 \\\\\n",
    "    0 & 1 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "Then the inverse of A would be leftward shear that pushes $\\hat{j}$ one unit to the left.\n",
    "$$\\left[\\begin{array}{cc}\n",
    "    1 & -1 \\\\\n",
    "    0 & 1 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "    \n",
    "In general, $A^{-1}$ is the unique transformation with the property that if you first apply $A$, then follow it with the transformation $A^{-1}$, you end up back where you started. Applying one transformation after another is captured algebraically with matrix multiplication, so the core property of this transformation $A^{-1}$ is that $A^{-1} * A$ equals the matrix that corresponds to doing nothing. The transformation that does nothing is called the **identity transformation**. It leaves $\\hat{i}$ and $\\hat{j}$ each where they are, unmoved.\n",
    "\n",
    "$$\\left[\\begin{array}{cc}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "    \n",
    "Once you find this inverse, which in practice (always with a computer). You can solve your equation by multiplying this inverse matrix by $\\vec{v}$ \n",
    "\n",
    "$$A^{-1}A\\vec{v} = A^{-1}\\vec{v}$$\n",
    "\n",
    "What this means geometrically is that you're playing the transformation in reverse and following $\\vec{v}$. This nonzero determinant case, which for a random choice of matrix is by far the most likely one, corresponds with the idea that if you have two unknowns and two equations (as below), its almost certainly the case that there's a single, unique solution, and also makes sense in higher dimensions, when the number of equations equals the number of unknowns. \n",
    "\n",
    "$$ \\begin{align}\n",
    "    ax + cy & = e \\\\\n",
    "    bx + dy & = f \\\\\n",
    "    \\end{align}$$\n",
    "    \n",
    "Again, the system of equations can be translated to the geometric interpretation, where you have some transformation, A, and some vector $\\vec{v}$, and you're looking for the vector $\\vec{x}$ that lands on $\\vec{v}$. As long as the transformation A doesn't squish all of space into a lower dimension, meaning its determinant is nonzero, there will be an inverse transformation, $A^{-1}$, with the property that if you first do $A$, then $A^{-1}$, its the same as doing nothing. And to solve your equation, you just have to multiply that reverse transformation matrix by the vector $\\vec{v}$. \n",
    "\n",
    "But when the **determinant is zero**, and the transformation associated with this system of equations squishes space into a smaller dimension, there is no inverse. $\\text{det}\\left(A\\right) = 0$, you cannot un-squish a line to turn it into a plane. _At least no function can do this_. That would require transforming each individual vector into a whole line full of vectors. but functions can only take a single input to a single output. Similiarly, for three equations in three unknowns, there will be no inverse if the corresponding transformation, squishes 3D space onto the plane, or even if it squishes it onto a line or point. Those all correspond to a determinant of zero, since any region is squishes into something with zero volume.\n",
    "\n",
    "It is still possible that a solution exists even when there is no inverse, its just that when your transformation squishes space onto, say, a line, you have to be lucky enough that the vector $\\vec{v}$ lives somewhere on that line. You might notice that some of these zero determinant cases feel a lot more restrictive than others. **Rank** - When the output of a transformation is a line, meaning it's one-dimensional, we say the transformation has a **rank** of 1. If all the vectors land on some two-dimensional plane, we say the transformation has a **rank** of 2. **Rank means the number of dimensions in the output of a transformation**.\n",
    "\n",
    "For instance, in the case of a 2x2 matrices, rank 2 is the best that it can be, it means the basis vectors continue to span the full two dimensions of space, and the determinant is nonzero. But for 3x3 matrices, rank 2 means that we've collapsed, but not as much as they would have collapsed for a rank 1 situation. If a 3D transformation has a nonzero determinant and its output fills all of 3D space, it has a rank of 3. This set of all possible outputs for your matrix, whether its a line, a plane, 3D space, whatever, is called the **Column Space** of your matrix. \n",
    "\n",
    "**Where does the name come from?**\n",
    "The columns of your matrix tell you where the basis vectors land, $\\hat{i}$ the first column and $\\hat{j}$ the second column, and the span of those transformed basis vectors gives you all possible outputs. \n",
    "$$\\left[\\begin{array}{cc}\n",
    "    3 & 1 \\\\\n",
    "    4 & 1 \\\\\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "In other words, the column space is the span of the columns of your matrix. So a more precise definition of rank would be that its the number of dimensions in the column space. When the rank is as high as it can be, meaning it equals the number of columns, we call the matrix **full rank**. Notice, the zero vector will always be included in the column space, since linear transformations must keep the origin fixed in place. For a full rank transformation, the only vector that lands at the origin is the zero vector itself, but for matrices that aren't full rank, which squish to a smaller dimension, you can have a whole bunch of vectors that land on zero. If a 2D transformation squishes space onto a line, for example, there is a separate line in a different direction, full of vectors that get squished onto the origin. If a 3D transformation squishes space onto a plane, there's also a full line of vectors that land on the origin. If a 3D transformation squishes space onto a line, then theres a whole plane full of vectors that land on the origin. This set of vectors that lands on the origin is called the **Null Space** or the **Kernel** of your matrix. Its the space of all vectors that become null, in the sense that they land on the zero vector. \n",
    "\n",
    "In terms of the linear system of equations, when $\\vec{v}$ happens to be the zero vector, the null space gives you all of the possible solutions to the equation. \n",
    "\n",
    "$$A\\vec{v} = \\left[\\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array}\\right]$$\n",
    "\n",
    "\n",
    "#### Overview\n",
    "This is a high-level overview of how to think about linear systems of equations geometrically. Each system has some kind of linear transformaton associated with it and when that transformation has an inverse, you can use that inverse to solve your system. Otherwise, the idea of columns space allows us to understand when a solution even exists, and the idea of a null space helps us to understand what the set of all possible solutions can look like. \n",
    "\n",
    "### But how do we calculate these things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
