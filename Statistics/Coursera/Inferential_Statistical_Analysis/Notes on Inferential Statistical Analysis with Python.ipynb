{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Bayesian Statistics\n",
    "\n",
    "There are two primary frameworks within Statistics, they are frequentist and Bayesian. \n",
    "\n",
    "In frequentists statistics, probabilites are made of the world. So, whether something is actually correct or incorrect. But in Bayesian statistics, probabilities are made in your mind (for the game involved). Thus, we can update our probabilities when we get new information, as long as the game has not been finished. \n",
    "\n",
    "**to be continued**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology and Notation\n",
    "\n",
    "**Averages or Means**\n",
    "- $\\mu$ = Population Mean Parameter\n",
    "- $\\hat{\\mu} \\text{ or } \\bar{x}$ = Sample Mean\n",
    "\n",
    "**Variability**\n",
    "- $\\sigma$ = Population Standard Deviation Parameter\n",
    "- $\\hat{\\sigma} \\text{ or } \\mathbf{s}$ = Sample Standard Deviation\n",
    "\n",
    "**Proportions**\n",
    "- $\\pi \\text{ and } \\hat{\\pi} \\ \\text{or} \\ p \\text{ and } \\hat{p}$ = Population Proportions and Sample Proportions\n",
    "\n",
    "**Confidence Intervals**\n",
    "Base Estimate +/- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a Difference in Population Means with Confidence (for independent groups)\n",
    "\n",
    "Research question for Mexican-American adults (18-29) living in US, do males and females differ significantly in Body Mass Index (BMI)\n",
    "\n",
    "- **Population**: Mexican American Adults (ages 18-29) in the US\n",
    "- **Parameter of Interest $(\\mu_1 - \\mu_2)$**: Body Mass Index or BMI(kg/$m^2$)\n",
    "\n",
    "We compare the two means of the population, which we will use our sample means, which will provide us our best estimate to start out with. \n",
    "\n",
    "### Sampling Distribution of the difference in two independent sample means\n",
    "\n",
    "If models for both populations of responses are approximately normal (or sample sizes are both large enough), distributon of the difference in sample means is (approximately) normal. \n",
    "\n",
    "The sample mean will change from sample to sample, this will help us create our sampling distribution. \n",
    "\n",
    "**Note** The same difference in two sample means can get a little wordy at times, so if we say a sample statistic, this is going to reference that difference in two sample means. So for the sampling distribution of our sample statistic, which will vary from sample to sample, and that will vary around our true population mean difference, which is $(\\mu_1 - \\mu_2)$.\n",
    "\n",
    "How much will that vary will determine on our **standard error**. On average it will tell us how much our sample statistic will fall from our true parameter. \n",
    "\n",
    "Standard Error = $\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$\n",
    "\n",
    "We do not know the true values of the population varianes denoted by $\\sigma$, so the best we're going to estimate this with our sample variances. \n",
    "\n",
    "Estimated Standard Error = $\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\n",
    "\n",
    "\n",
    "### Confidnce Interval Basics\n",
    "\n",
    "#### Best Estimate +/- Margin of Error\n",
    "\n",
    "As we have seen before the confidence interval will start with the midpoint as our best estimate and then add and subtract a margin of error to get both sides. \n",
    "\n",
    "**Best estimate** = Unbiased Point Estiamte, in this example its the difference of our sample means. \n",
    "\n",
    "**Margin of Error** = \"a few\" Estimated Standard Errors\n",
    "\n",
    "A few will be a multiplier from appropriate distriution based on desired confidence level and sample design\n",
    "\n",
    "95% Confidence Level 0.05 Significance\n",
    "\n",
    "For two means, we're going to use a T-Distribution and for this example we're going to be using a 95% confidence level, which is equivalent to a 5% significance level in a hypothesis test. \n",
    "\n",
    "### Confidence Interval Approaches\n",
    "\n",
    "**Pooled Approach**\n",
    "\n",
    "The variance of the two populations are assumed to be equal. ($\\sigma_1^2 = \\sigma_2^2$)\n",
    "\n",
    "**Unpooled Approach**\n",
    "\n",
    "The assumption of equal variances is dropped\n",
    "\n",
    "### Unpooled Confidence Interval Calculations\n",
    "\n",
    "**Best Estimate $\\pm$ Margin of Error**\n",
    "\n",
    "Difference in sample means $\\pm$ \"a few\" estimated standard error\n",
    "\n",
    "$$(\\bar{x_1} - \\bar{x_2}) \\pm t^* \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$$\n",
    "\n",
    "The t star multiplier, the df for the t* multiplier can be found using Welch's approximation. You can also take a conservative approach by taking the smaller of n1 - 1 or n2 - 1, i.e. df = min(n1-1, n2-1).\n",
    "\n",
    "### Pooled Confidence Interval Calculations\n",
    "\n",
    "**Best Estimate $\\pm$ Margin of Error**\n",
    "\n",
    "Difference in sample means $\\pm$ \"a few\" estimated standard error\n",
    "\n",
    "This is when our sample variances are close enough that we can assume the population variances are close as well. Then we pool that variance together, the estimated standard error for pooled will change now.\n",
    "\n",
    "\n",
    "$$(\\bar{x_1} - \\bar{x_2}) \\pm \\text{t*} \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2)}{n_1 + n_2-2}} \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}$$\n",
    "\n",
    "\n",
    "We use the same t* multiplier which comes from a t-distribution with n1 + n2 - 2 degrees of freedom, again, this approach can be used if we assume the population variances are equal. \n",
    "\n",
    "### Example from Before\n",
    "\n",
    "- First of our assumptions is that we have a simple random sample for both of our populations, males and females groups that we're looking at. Because of how NHANES is created we will say that this assumption is True. \n",
    "\n",
    "- Second of our assumptions is that we need both of these samples to be independent of one another. We will assume that is the case too. \n",
    "\n",
    "- The third assumption we need to check is called the **Normality Assumption**, models for both populations of responses are approximately normal (or sample sizes are both 'large' enough). Look at the frequency histograms to determine normality as well as Q-Q plots. In this example, both distribution have a slight-to-moderate right skew, but the large sample sizes let us apply the CLT and continue. CLT = Central Limit Theorem.\n",
    "\n",
    "- The last assumption we need to check is called the **Variance Assumption**, if we have enough evidence to assume equal variances betwen the two populations, we can use the \"pooled\" approach. To test this we can look at a box plot of both Genders and compare the IQRs from the box plot, or the interquartile range, and we can look at the sample standard deviations that we have. If the IQR's and the standard deviations are similar enough to make this assumptions, than the pooled approach will be used!\n",
    "\n",
    "$$(\\bar{x_1} - \\bar{x_2}) \\pm \\text{t*} \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2)}{n_1 + n_2-2}} \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}$$\n",
    "\n",
    "\n",
    "Male mean, std, n = 23.57, 6.24, 258\n",
    "Female mean, std, n = 22.83, 6.43, 239\n",
    "\n",
    "We will use a t* of 1.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.740000000000002, 1.98, 100.13412086806476, 0.08977788931946498)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sqrt(num):\n",
    "    return num ** (1/2)\n",
    "\n",
    "# \n",
    "(23.57 - 22.83) + 1.98 * sqrt((258-1)*(6.24**2) + (239-1)*(6.43**2) / (258 + 239 -2))\n",
    "\n",
    "\n",
    "\n",
    "(23.57 - 22.83), 1.98, sqrt((258-1)*6.24**2 + (239-1)*6.43**2 / (258 + 239 -2)), sqrt((1/258)+(1/239))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10006.9632"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "257 * (6.24**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9267.1488"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "238 * (6.24**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "258+239 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.9376"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10006.9632 + 9267.1488) / 495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.24"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(38.9376)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8380194975327864"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.740000000000002 + (1.96 * 6.24 * 0.08977788931946498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3580194975327824"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.740000000000002 - (1.96 * 6.24 * 0.08977788931946498)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(\\bar{x_1} - \\bar{x_2}) \\pm t^* \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15092093023255815, 0.17299121338912132)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6.24**2)/258, (6.43**2) / 239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.126882943279572"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(258 - 239) + 1.98 * sqrt(0.15092093023255815 + 0.17299121338912132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1268829432795724"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.98 * sqrt(0.15092093023255815 + 0.17299121338912132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8668829432795744, -0.3868829432795704)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(23.57 - 22.83) + 1.1268829432795724, (23.57 - 22.83) - 1.1268829432795724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus what we can say is that with **95% confidence**, the difference in mean body mass index between males and femals for all Mexican-American adults (ages 18-29) in the US is **estimated** to be between -0.38 kg/$m^2$ and 1.86 kg/$m^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Confidence Level\n",
    "\n",
    "#### What does this \"with 95% confidence\" mean?\n",
    "\n",
    "If this procedure were repeated over and over each time producing a 95% confidence interval estimate, we would **expect 95% of those resulting intervals to contain the difference in population mean BMI**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Confidence intervals are used to give an interval estimate for our parameter of interest **difference in population means**\n",
    "- Center of the Confidence Interval is our best estimate **difference in sample means**\n",
    "- Margin of Error is \"a few\" (estimated) standard errors **for two means we use t* multipliers (pooled vs unpooled)**\n",
    "    - data are two simple random samples, independent from one another\n",
    "    - both populations of responses are normal (if we dont have normality large n helps and we can use the CLT as in the above example), than check for equal variances which determines our pooled vs unpooled. \n",
    "- Know how to interpret the **interval** we find and the **confidence level**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "CLT states that the sample mean of independent and identically distributed values will be approximately normally distributed. The CLT also implies that the Z-score will be approximately normally distributed. Importantly, the CLT provides these guarantees even when the individual data values have distriutions that are not normal, as long as the sample size is \"sufficientyl large.\" There are some additional technical conditions. \n",
    "\n",
    "## Alternative procedures for challenging situations\n",
    "\n",
    "There are a few ways to reduce the risk that strong non-normality will lead to confidence intervals with poor performance. In order to provide some exposire to the types of procedures that statisticians use to conduct inference in challenging situations.\n",
    "\n",
    "When working with the sample proportions, it is common to add two extra “successes” and two extra “failures” to the data before calculating the proportion. Thus, if we observe 5 successes and 7 failures, instead of estimating the success rate as 5 / (5 + 7), we estimate it as 7 / (7 + 9). The standard error is also estimated using this adjustment. The resulting confidence interval generally has better coverage properties than the usual CI when the sample size is small. This interval is often called the “Agresti-Coull” interval, after its inventors.\n",
    "\n",
    "When working with strongly skewed data, another practical technique for improving the coverage properties of intervals is to transform the data with a skew-reducing transformation, e.g. a log transformation, then calculate the interval in the usual way (as described above) using the transformed data. The resulting interval can be transformed back to the original scale by applying the inverse transformation to the LCB and UCB. For example, if the transformation is the natural logarithm, the inverse transformation would be to exponentiate (anti-log) the LCB and UCB.\n",
    "\n",
    "## Conclusion\n",
    "In summary, although normality of the data can play a role in determining the coverage properties of a confidence interval, it is generally not a major factor unless the sample size is quite small (much smaller than 50), or if the data are strongly non-normal. In most cases, other factors besides Gaussianity of the individual data values are more likely to give rise to sub-optimal coverage. Two such factors that can cause major problems with CI coverage probabilities are clustering or other forms of dependence in the data, and overt or hidden pre-testing or multiplicity in the analysis. Clustering will be discussed extensively in Course 3. We will discuss multiplicity in Week 3 of this course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Affects the Standard error of an Estimate?\n",
    "\n",
    "The standard error provides us with an estimate of the standard deviation of the sampling distribution of estimates that would arise if we had drawn repeated samples of the same size and computer the same estimate for each random sample. In a simplified sense, the standard error gives us a sense of uncertainty associated with our estimate. Estimates with smaller standard errors are thus considered more precise.\n",
    "\n",
    "What exactly impacts a standard error in terms of a study design?\n",
    "\n",
    "1. The variance of the variables of interest that are used to computer the estimate.\n",
    "\n",
    "In general, the more variability that is associated with a given variable being measured, the more imprecise estimates based on that variable will be. This makes careful and precise measurement of the variables of interest very important for a given study. \n",
    "\n",
    "2. The size of the sample. \n",
    "\n",
    "Larger samples will tend to produce sampling distributions with less variability (or, estimates with smaller standard errors). The more sample that can be measured, the better but just because we have a set of \"big data\" does not mean that we have a collection of precise measurements. Very unusual measures (outliers) could have strong influence on the variance of a given variable.\n",
    "\n",
    "3. The amount of dependence in the observations collected, possibly due to cluser sampling. \n",
    "\n",
    "We need to ensure that the data collection will be entirely independent, especially when cluser sampling. Studies were clusters of units with similar characteristics are measures, data will not be entirely independent within such cluster (neighborhood, clinic, school etc.). This is because they are coming from the same cluser and will generally have similar values on the variables of interest, this could happen for a variety of reasons. This lack of independence in the observations collected reduces our **effective sample size**. we dont have as much unique information as the size of the same suggests. While the sample may look large, most of the observations will be strongly correlated with each other, and we need to _account for this_. In general, with these types of clustered data, standard errors will tend to be much larger, becasue the estimates computed across different studies will entirely depend on what clusters are under the study. The larger the sample size selected from each cluser (and thus the smaller the sample of clusters), the larger the standard errors will tend to be.\n",
    "\n",
    "4. The stratification of the target sample. \n",
    "\n",
    "If we select a **stratified sample** from a target population, we will tend to produce estimates with increased precision, because we are removing between-stratum variance from the variability of our estimates by design! Stratification of samples is always an imporant consideration. \n",
    "\n",
    "5. The use of sampling weights to compute our estimates\n",
    "\n",
    "While sampling weights are often necessary to compute unbiased population estimates, the use of weights in estimation can inflate the variance of our estimates. We can use specialized statistical procedures to make sure that our standard errors reflect the uncertainty in our estimates due to weighting. In general, the higher the variability in our weights, the more variable our estimates will be.\n",
    "\n",
    "These five features are generally the main drivers of standard errors, but other design features may also ultimately affect standard errors (e.g., imputation of missing data). We will touch on these throughout the specialization.\n",
    "\n",
    "\n",
    "https://www.coursera.org/learn/inferential-statistical-analysis-python/supplement/3eBYa/what-affects-the-standard-error-of-an-estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
